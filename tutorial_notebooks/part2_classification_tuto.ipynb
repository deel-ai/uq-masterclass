{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba4c9c2",
   "metadata": {
    "id": "cba4c9c2"
   },
   "source": [
    "# üìä Part II: Conformal Classification\n",
    "\n",
    "The objective is to use conformal prediction to assess the uncertainty associated with a predictive classifier. We will assume that this classifier is already deployed in production from PUNCC's perspective. Our task is to perform post-hoc calibration to enable the generation of reliable prediction intervals.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"assets/classification_demo.svg\" width=\"800\"> </img>\n",
    "</div>\n",
    "\n",
    "**Links**\n",
    "- [<img src=\"https://github.githubassets.com/images/icons/emoji/octocat.png\" width=20> Github](https://github.com/deel-ai/puncc)\n",
    "- [üìò Documentation](https://deel-ai.github.io/puncc/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e8c1e0",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Setup  <a class=\"anchor\" id=\"cr-setup\"></a>\n",
    "\n",
    "üêæ Ensure we have puncc installed. You can install it if needed using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc706a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install puncc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d757e5ca",
   "metadata": {},
   "source": [
    "We import some modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29028bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(0)\n",
    "tf.keras.utils.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30047277",
   "metadata": {
    "id": "30047277"
   },
   "source": [
    "### üíæ MNIST Dataset <a class=\"anchor\" id=\"cc-data\"></a>\n",
    "\n",
    "MNIST dataset contains a large number of $28\\times28$ digit images to which are associated digit labels. As the data generating process is considered i.i.d (check [this post](https://newsletter.altdeep.ai/p/the-story-of-mnist-and-the-perils)), conformal prediction is applicable üëè.\n",
    "\n",
    "We have two available data subsets:\n",
    "\n",
    "* Calibration subset ${\\cal D_{calib}}$ on which nonconformity scores are computed.\n",
    "* New data subset ${\\cal D_{new}}$ on which the prediction set are estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5c87be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d5c87be",
    "outputId": "c65ff91e-3bd6-42bc-cf45-7207c2b80f6f"
   },
   "outputs": [],
   "source": [
    "# Load MNIST Database\n",
    "(X_train_mnist, y_train_mnist), (X_new_mnist, y_new_mnist) = (\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    ")\n",
    "\n",
    "# Preprocessing: reshaping and standardization\n",
    "X_train_mnist = X_train_mnist.reshape((len(X_train_mnist), 28, 28))\n",
    "X_train_mnist = X_train_mnist.astype(\"float32\") / 255\n",
    "X_new_mnist = X_new_mnist.reshape((len(X_new_mnist), 28, 28))\n",
    "X_new_mnist = X_new_mnist.astype(\"float32\") / 255\n",
    "\n",
    "# Split fit and calib datasets\n",
    "X_fit_mnist = X_train_mnist[:50000]\n",
    "y_fit_mnist = y_train_mnist[:50000]\n",
    "\n",
    "# Calibration data\n",
    "X_calib_mnist = X_train_mnist[50000:]\n",
    "y_calib_mnist = y_train_mnist[50000:]\n",
    "\n",
    "# One hot encoding of classes\n",
    "y_fit_cat = tf.keras.utils.to_categorical(y_fit_mnist)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(5, 5))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i].imshow(1 - X_fit_mnist[i], cmap=\"gray\")\n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c4d98",
   "metadata": {
    "id": "027c4d98"
   },
   "source": [
    "### üîÆ Prediction model <a class=\"anchor\" id=\"cc-pm\"></a>\n",
    "\n",
    "We will consider a convolutional neural network (convnet) defined below. The model will be trained prior to any conformalization and will be assumed to be in production from this point on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e5a49",
   "metadata": {
    "id": "739e5a49"
   },
   "outputs": [],
   "source": [
    "# Classification model: convnet composed of two convolution/pooling layers\n",
    "# and a dense output layer\n",
    "convnet_model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Conv2D(16, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "convnet_model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "history = convnet_model.fit(\n",
    "    X_fit_mnist,\n",
    "    y_fit_cat,\n",
    "    epochs=1,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3127fb0",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Naive Approach <a class=\"anchor\" id=\"cc-calib\"></a>\n",
    "\n",
    "Our convnet has softmax function applied to logit outputs, to normalize them and convert them into probabilities. One may attempt to use logit scores to meet requirements on error rate $\\le \\alpha = 0.05$.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"assets/naive_classification.svg\" width=\"600\"> </img>\n",
    "</div>\n",
    "\n",
    "These logits can be used to determine the class with the highest score, but they do not necessarily represent well-calibrated probabilities.\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"assets/fooling_dnns.png\" width=\"800\"> </img>\n",
    "</div>\n",
    "\n",
    "We can't trust logit scores to reliably estimate uncertainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdfb0d",
   "metadata": {
    "id": "f0fdfb0d"
   },
   "source": [
    "### ‚öôÔ∏è Conformal Prediction <a class=\"anchor\" id=\"cc-cp\"></a>\n",
    "\n",
    "The goal is provide a reliable uncertainty evaluation through conformal prediction associated with our pretrained convnet classifier.\n",
    "\n",
    "The APS procedure is the chosen method.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"assets/workflow_classification.svg\" width=\"800\"> </img>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe6c5e0",
   "metadata": {},
   "source": [
    "#### 1. Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73525e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deel.puncc.api.prediction import BasePredictor\n",
    "from deel.puncc.classification import APS\n",
    "\n",
    "# Instanciate the APS wrapper around the convnet predictor.\n",
    "# The `train` argument is set to False as the model is already trained\n",
    "convnet_cp = APS(..., train=...)  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf5fba",
   "metadata": {},
   "source": [
    "#### 2. Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815727db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the nonconformity scores on the calibration dataset\n",
    "convnet_cp.fit(X_calib=..., y_calib=...)  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8291b16a",
   "metadata": {},
   "source": [
    "#### 3. Conformal Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use chooses the coverage target 1-alpha = 95%\n",
    "alpha = 0.05\n",
    "\n",
    "# The `predict` returns the output of the convnet model `y_pred` and\n",
    "# the calibrated prediction set `set_pred`.\n",
    "y_pred, set_pred = convnet_cp.predict(..., alpha=...)  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae842f7d",
   "metadata": {
    "id": "ae842f7d"
   },
   "source": [
    "Let's visualize an example of point prediction and set prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6e674",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "3db6e674",
    "outputId": "5f2ff029-6027-47b7-e0a2-373d362282dd"
   },
   "outputs": [],
   "source": [
    "sample = 18\n",
    "\n",
    "# sort y_pred[sample] in reversed order\n",
    "ranked_pred = y_pred[sample].argsort()[::-1]\n",
    "\n",
    "# Plot results\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.imshow(X_new_mnist[sample].reshape((28, 28)), cmap=\"gray\")\n",
    "_ = plt.title(\n",
    "    f\"Point prediction: {np.argmax(y_pred[sample])} \\n \"\n",
    "    + f\"Prediction set: {set_pred[sample]} \\n True label: {y_new_mnist[sample]}\"\n",
    ")\n",
    "\n",
    "_ = plt.xticks([])\n",
    "_ = plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781fcee",
   "metadata": {
    "id": "6781fcee"
   },
   "source": [
    "Puncc provides several metrics in `deel.puncc.metrics` to evaluate the conformalization procedure. Below, we compute the average empirical coverage and the average empirical size of the prediction sets on the new examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dbd478",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89dbd478",
    "outputId": "29c0d131-95fb-44ee-87d8-1bfd63b76fa1"
   },
   "outputs": [],
   "source": [
    "from deel.puncc import metrics\n",
    "\n",
    "mean_coverage = metrics.classification_mean_coverage(y_new_mnist, set_pred)\n",
    "mean_size = metrics.classification_mean_size(set_pred)\n",
    "\n",
    "print(f\"Empirical coverage : {mean_coverage:.2f}\")\n",
    "print(f\"Average set size : {mean_size:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88129c4f",
   "metadata": {},
   "source": [
    "We can check the calibrated threshold selected by the algorithm (quantile of nonconformity scores) and conclude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5641b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nonconformity scores\n",
    "nonconf_scores = convnet_cp.conformal_predictor.get_nonconformity_scores()[0]\n",
    "\n",
    "# Size of the calibration set\n",
    "n = len(nonconf_scores)\n",
    "\n",
    "# Compute the calibrated treshold\n",
    "calibrated_treshold = np.quantile(\n",
    "    nonconf_scores, (1 - alpha) * (n + 1) / n, method=\"inverted_cdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Uncalibrated treshold : {1-alpha:.2f}\")\n",
    "print(f\"Calibrated treshold : {calibrated_treshold:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "punc-user-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
